{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e66e7cd",
   "metadata": {},
   "source": [
    "# Topic modeling using BERTopic\n",
    "## Introduction\n",
    "As an alternative to the previously presented approaches LDA and NMF, we can also use a transformer-based approach. For this example, we will be using BERTopic as our model, and compare if we can get meaingful results with it as well.\n",
    "\n",
    "Goal of this experiment is to find out, if we can reduce time neede to preprocess the data for traditional approaches (as seen in the previos notebook)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799eaf53",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "Quote from [BERTopic GitHub](https://github.com/MaartenGr/BERTopic/issues/40):\n",
    "\n",
    "In general, no, you do not need to preprocess your data. Like you said, keeping the original structure of the text is especially important for transformer-based models to understand the context.\n",
    "However, there are exceptions to this. For example, if you were to have scraped documents with a lot of html tags, then it might be beneficial to remove those as they do not provide any interesting context.\n",
    "If you have paragraphs in a document, then it might be worthwhile to split up the paragraphs in order to more precisely extract the correct topics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a79abc",
   "metadata": {},
   "source": [
    "### Remaining tasks\n",
    "Since the protocols are not HTML-based, we do not need to strip tags from it. However, we do have a big number of paragraphs in the dataset, so before model training we are splitting them up in the declared train_bertopic method.\n",
    "\n",
    "As the documentation suggests to use a count_vectorizer to remove stopwords, we pass that to the model creation as well.\n",
    "\n",
    "Most code was already explained previously, new is the UMAP model: It helps reducing the dimensions for the analyzing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3eef1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from umap import UMAP\n",
    "\n",
    "BASE_INPUT = \"../../data/dataPreprocessedStage/speechContentCleaned\"\n",
    "BASE_OUTPUT = \"../../data/dataTopicModeling/bertopic\"\n",
    "\n",
    "umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine')\n",
    "\n",
    "def train_bertopic(input_pickle, output_npy, model, vectorizer_model):\n",
    "\tdf = pd.read_pickle(input_pickle)\n",
    "\tparagraphs = [\n",
    "\t\tparagraph.strip()\n",
    "\t\tfor paragraphs in df['speech_content'].str.split('\\n')\n",
    "\t\tfor paragraph in paragraphs\n",
    "\t\tif paragraph.strip()\n",
    "\t]\n",
    "\t# check if output_npy exists, and if not train embeddings\n",
    "\tif os.path.exists(output_npy):\n",
    "\t\tembeddings = np.load(output_npy)\n",
    "\t\tprint(f\"Embeddings already exist at {output_npy}, skipping training.\")\n",
    "\telse:\n",
    "\t\tprint(f\"Training embeddings for {len(paragraphs)} paragraphs...\")\n",
    "\t\tembeddings = train_embeddings(output_npy, paragraphs, model)\n",
    "\t# now check if trained bertopic model has already been saved\n",
    "\tif os.path.exists(output_npy.replace('.npy', '.model')):\n",
    "\t\tprint(f\"BERTopic model already exists at {output_npy.replace('.npy', '.model')}, skipping training.\")\n",
    "\t\treturn\n",
    "\t# train BERTopic model\n",
    "\tprint(\"Training BERTopic model...\")\n",
    "\t# create BERTopic model with the specified vectorizer and UMAP model\n",
    "\t# Note: calculate_probabilities=False to speed up training, probabilities can be calculated later if needed\n",
    "\t# Note: language=\"german\" to use the German stop words from the CountVectorizer\n",
    "\tif not os.path.exists(os.path.dirname(output_npy)):\n",
    "\t\tos.makedirs(os.path.dirname(output_npy))\n",
    "\t# create BERTopic model with the specified vectorizer and UMAP model\n",
    "\ttopic_model = BERTopic(vectorizer_model=vectorizer_model, language=\"german\", verbose=True, calculate_probabilities=False, umap_model=umap_model)\n",
    "\tprint(f\"len(paragraphs): {len(paragraphs)}\")\n",
    "\tprint(f\"embeddings.shape: {embeddings.shape}\")\n",
    "\ttopics = topic_model.fit_transform(paragraphs, embeddings=embeddings)\n",
    "\t# save the model\n",
    "\ttopic_model.save(output_npy.replace('.npy', '.model'))\n",
    "\t# save topics as well\n",
    "\tnp.save(output_npy.replace('.npy', '_topics.npy'), topics)\n",
    "\t#np.save(output_npy.replace('.npy', '_probs.npy'), probs)\n",
    "\tprint(f\"BERTopic model trained and saved to {output_npy.replace('.npy', '.model')}\")\n",
    "\t#return topic_model, embeddings\n",
    "\n",
    "def train_embeddings(output_npy, paragraphs, model):\n",
    "\tprint(f\"Number of threads used: \", torch.get_num_threads())\n",
    "\tembedding_model = SentenceTransformer(model, backend='torch')\n",
    "\tembeddings = embedding_model.encode(paragraphs, show_progress_bar=True, batch_size=128)\n",
    "\tos.makedirs(os.path.dirname(output_npy), exist_ok=True)\n",
    "\tnp.save(output_npy, embeddings)  # speichern!\n",
    "\treturn embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210ea325",
   "metadata": {},
   "source": [
    "### Training\n",
    "We will now train the model on the data of the 19th and 20th electural terms, just like we did with LDA and NMF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77cd7970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings already exist at bertopic/iteration_1/embeddings_19.npy, skipping training.\n",
      "BERTopic model already exists at bertopic/iteration_1/embeddings_19.model, skipping training.\n",
      "Embeddings already exist at bertopic/iteration_1/embeddings_20.npy, skipping training.\n",
      "BERTopic model already exists at bertopic/iteration_1/embeddings_20.model, skipping training.\n"
     ]
    }
   ],
   "source": [
    "german_stop_words = stopwords.words('german')\n",
    "nltk_vectorizer_model = CountVectorizer(stop_words=german_stop_words)\n",
    "\n",
    "for term in [19, 20]:\n",
    "    # define paths\n",
    "    input_pickle = os.path.join(BASE_INPUT, f\"speech_content_cleaned_{term}.pkl\")\n",
    "    output_npy = os.path.join(BASE_OUTPUT, f\"iteration_1/embeddings_{term}.npy\")\n",
    "    train_bertopic(input_pickle, output_npy, model=\"paraphrase-multilingual-MiniLM-L12-v2\", vectorizer_model=nltk_vectorizer_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04a6ab71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>156871</td>\n",
       "      <td>-1_herr_deutschen_deutschland_schon</td>\n",
       "      <td>[herr, deutschen, deutschland, schon, kollegen...</td>\n",
       "      <td>[Ich glaube, dass wir im Deutschen Bundestag b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>27468</td>\n",
       "      <td>0_büttenrede_verwechseln_ach_nein</td>\n",
       "      <td>[büttenrede, verwechseln, ach, nein, mal, scho...</td>\n",
       "      <td>[({0}), ({0}), ({0})]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>22544</td>\n",
       "      <td>1_entscheidungssatz_leere_läuft_daher</td>\n",
       "      <td>[entscheidungssatz, leere, läuft, daher, völli...</td>\n",
       "      <td>[({1}), ({1}), ({1})]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>20201</td>\n",
       "      <td>2____</td>\n",
       "      <td>[, , , , , , , , , ]</td>\n",
       "      <td>[({2}), ({2}), ({2})]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>18020</td>\n",
       "      <td>3_ziel___</td>\n",
       "      <td>[ziel, , , , , , , , , ]</td>\n",
       "      <td>[({3}), ({3}), ({3})]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1179</th>\n",
       "      <td>1178</td>\n",
       "      <td>10</td>\n",
       "      <td>1178_wertpapierinstituten_wertpapierfirmen_ric...</td>\n",
       "      <td>[wertpapierinstituten, wertpapierfirmen, richt...</td>\n",
       "      <td>[Jetzt ist die Frage, wie wir das innerhalb de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1180</th>\n",
       "      <td>1179</td>\n",
       "      <td>10</td>\n",
       "      <td>1179_hartz_iv_damoklesschwert_paternalistisches</td>\n",
       "      <td>[hartz, iv, damoklesschwert, paternalistisches...</td>\n",
       "      <td>[Aber die Analyse, die Sie in Ihrem Antrag vor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1181</th>\n",
       "      <td>1180</td>\n",
       "      <td>10</td>\n",
       "      <td>1180_pestizide_pestiziden_exportiert_doppelsta...</td>\n",
       "      <td>[pestizide, pestiziden, exportiert, doppelstan...</td>\n",
       "      <td>[Da ich jetzt noch ein bisschen Zeit habe: Auc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1182</th>\n",
       "      <td>1181</td>\n",
       "      <td>10</td>\n",
       "      <td>1181_vögel_fledermäuse_insekten_lungen</td>\n",
       "      <td>[vögel, fledermäuse, insekten, lungen, windkra...</td>\n",
       "      <td>[Die Menschen, die im Umfeld von Windindustrie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1183</th>\n",
       "      <td>1182</td>\n",
       "      <td>10</td>\n",
       "      <td>1182_ups_start_skalierbare_gründergeist</td>\n",
       "      <td>[ups, start, skalierbare, gründergeist, kreati...</td>\n",
       "      <td>[Vielen Dank. – Herr Präsident! Meine Damen un...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1184 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Topic   Count                                               Name  \\\n",
       "0        -1  156871                -1_herr_deutschen_deutschland_schon   \n",
       "1         0   27468                  0_büttenrede_verwechseln_ach_nein   \n",
       "2         1   22544              1_entscheidungssatz_leere_läuft_daher   \n",
       "3         2   20201                                              2____   \n",
       "4         3   18020                                          3_ziel___   \n",
       "...     ...     ...                                                ...   \n",
       "1179   1178      10  1178_wertpapierinstituten_wertpapierfirmen_ric...   \n",
       "1180   1179      10    1179_hartz_iv_damoklesschwert_paternalistisches   \n",
       "1181   1180      10  1180_pestizide_pestiziden_exportiert_doppelsta...   \n",
       "1182   1181      10             1181_vögel_fledermäuse_insekten_lungen   \n",
       "1183   1182      10            1182_ups_start_skalierbare_gründergeist   \n",
       "\n",
       "                                         Representation  \\\n",
       "0     [herr, deutschen, deutschland, schon, kollegen...   \n",
       "1     [büttenrede, verwechseln, ach, nein, mal, scho...   \n",
       "2     [entscheidungssatz, leere, läuft, daher, völli...   \n",
       "3                                  [, , , , , , , , , ]   \n",
       "4                              [ziel, , , , , , , , , ]   \n",
       "...                                                 ...   \n",
       "1179  [wertpapierinstituten, wertpapierfirmen, richt...   \n",
       "1180  [hartz, iv, damoklesschwert, paternalistisches...   \n",
       "1181  [pestizide, pestiziden, exportiert, doppelstan...   \n",
       "1182  [vögel, fledermäuse, insekten, lungen, windkra...   \n",
       "1183  [ups, start, skalierbare, gründergeist, kreati...   \n",
       "\n",
       "                                    Representative_Docs  \n",
       "0     [Ich glaube, dass wir im Deutschen Bundestag b...  \n",
       "1                                 [({0}), ({0}), ({0})]  \n",
       "2                                 [({1}), ({1}), ({1})]  \n",
       "3                                 [({2}), ({2}), ({2})]  \n",
       "4                                 [({3}), ({3}), ({3})]  \n",
       "...                                                 ...  \n",
       "1179  [Jetzt ist die Frage, wie wir das innerhalb de...  \n",
       "1180  [Aber die Analyse, die Sie in Ihrem Antrag vor...  \n",
       "1181  [Da ich jetzt noch ein bisschen Zeit habe: Auc...  \n",
       "1182  [Die Menschen, die im Umfeld von Windindustrie...  \n",
       "1183  [Vielen Dank. – Herr Präsident! Meine Damen un...  \n",
       "\n",
       "[1184 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bertopic import BERTopic\n",
    "\n",
    "umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine')\n",
    "german_stop_words = stopwords.words('german')\n",
    "nltk_vectorizer_model = CountVectorizer(stop_words=german_stop_words)\n",
    "\n",
    "topics = np.load(os.path.join(BASE_OUTPUT, \"iteration_3/embeddings_19_topics.npy\"), allow_pickle=True)\n",
    "topic_model = BERTopic.load(os.path.join(BASE_OUTPUT, \"iteration_1/embeddings_19.model\"))\n",
    "# show topics\n",
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acab3e1d",
   "metadata": {},
   "source": [
    "## Outcome\n",
    "As can be seen in the table above, the model generated a table of topics. However, output is not yet precise and meaningful enough to get a detailed overview of the topics. Therefore, we use a bigger embedding model and compare the differences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c633a52c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings already exist at dataProcessedStage/topicModeling/paraphrase-multilingual-mpnet-base-v2/embeddings_19.npy, skipping training.\n",
      "BERTopic model already exists at dataProcessedStage/topicModeling/paraphrase-multilingual-mpnet-base-v2/embeddings_19.model, skipping training.\n",
      "Embeddings already exist at dataProcessedStage/topicModeling/paraphrase-multilingual-mpnet-base-v2/embeddings_20.npy, skipping training.\n",
      "BERTopic model already exists at dataProcessedStage/topicModeling/paraphrase-multilingual-mpnet-base-v2/embeddings_20.model, skipping training.\n",
      "Embeddings already exist at dataProcessedStage/topicModeling/paraphrase-multilingual-mpnet-base-v2/embeddings_19_20.npy, skipping training.\n",
      "Training BERTopic model...\n",
      "len(paragraphs): 1001726\n",
      "embeddings.shape: (1001726, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-15 13:16:20,832 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mDer Kernel ist beim Ausführen von Code in der aktuellen Zelle oder einer vorherigen Zelle abgestürzt. \n",
      "\u001b[1;31mBitte überprüfen Sie den Code in der/den Zelle(n), um eine mögliche Fehlerursache zu identifizieren. \n",
      "\u001b[1;31mKlicken Sie <a href='https://aka.ms/vscodeJupyterKernelCrash'>hier</a>, um weitere Informationen zu erhalten. \n",
      "\u001b[1;31mWeitere Informationen finden Sie unter Jupyter <a href='command:jupyter.viewOutput'>Protokoll</a>."
     ]
    }
   ],
   "source": [
    "for term in [19, 20, \"19_20\"]:\n",
    "\t# define paths\n",
    "\tmodel = \"paraphrase-multilingual-mpnet-base-v2\"\n",
    "\tinput_pickle = os.path.join(BASE_INPUT, f\"speech_content_cleaned_{term}.pkl\")\n",
    "\toutput_npy = os.path.join(BASE_OUTPUT, model, f\"embeddings_{term}.npy\")\n",
    "\ttrain_bertopic(input_pickle, output_npy, model, vectorizer_model=nltk_vectorizer_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6739e605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe tex2jax_ignore\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>162398</td>\n",
       "      <td>-1_herr_antrag_müssen_afd</td>\n",
       "      <td>[herr, antrag, müssen, afd, deutschland, schon, sagen, mal, ja, heute]</td>\n",
       "      <td>[– Gut. Jetzt lassen Sie mich doch mal darüber reden., Ich gebe der Opposition recht, die uns vorwirft, dass wir sehr lange gebraucht haben etc. Da haben Sie einen Punkt gemacht. Wir sind in einer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>27469</td>\n",
       "      <td>0_nullkommagarnichts_büttenrede_verwechseln_ach</td>\n",
       "      <td>[nullkommagarnichts, büttenrede, verwechseln, ach, nein, mal, schon, , , ]</td>\n",
       "      <td>[({0}), ({0}), ({0})]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>22542</td>\n",
       "      <td>1____</td>\n",
       "      <td>[, , , , , , , , , ]</td>\n",
       "      <td>[({1}), ({1}), ({1})]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>20201</td>\n",
       "      <td>2____</td>\n",
       "      <td>[, , , , , , , , , ]</td>\n",
       "      <td>[({2}), ({2}), ({2})]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>18019</td>\n",
       "      <td>3____</td>\n",
       "      <td>[, , , , , , , , , ]</td>\n",
       "      <td>[({3}), ({3}), ({3})]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1186</th>\n",
       "      <td>1185</td>\n",
       "      <td>10</td>\n",
       "      <td>1185_auswärtigen_diplomaten_ortsfesten_amt</td>\n",
       "      <td>[auswärtigen, diplomaten, ortsfesten, amt, spezialisten, amtes, rotation, dienstposten, auslandsbezug, ausländerrechtes]</td>\n",
       "      <td>[Der diplomatische Dienst ist durch zwei Prinzipien gekennzeichnet: Die Angehörigen des diplomatischen Dienstes müssen zur Rotation bereit sein, und sie verstehen sich als Generalisten. Das heißt ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1187</th>\n",
       "      <td>1186</td>\n",
       "      <td>10</td>\n",
       "      <td>1186_aushandlungssystem_schimke_tarifverträge_lohnsenkungen</td>\n",
       "      <td>[aushandlungssystem, schimke, tarifverträge, lohnsenkungen, unternehmenszentralen, auffangböden, bargeldauszahlungen, osten, tarifverträgen, menschenrechtsthema]</td>\n",
       "      <td>[Ein weiterer Punkt ist: Dort, wo wir Regelungen haben – es wurde schon von Frau Schimke genannt, der ich sonst selten zustimme –, beispielsweise bei den Tarifverträgen, bei den Löhnen, haben wir ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1188</th>\n",
       "      <td>1187</td>\n",
       "      <td>10</td>\n",
       "      <td>1187_regierungsansprache_wohlstandsindex_hauptgegenstand_einprogrammiert</td>\n",
       "      <td>[regierungsansprache, wohlstandsindex, hauptgegenstand, einprogrammiert, premium, dazugeben, einigungsprozesses, ropäische, regionalbanken, partizipiert]</td>\n",
       "      <td>[Die Krise wird benutzt: zum einen, wie erwähnt, für den Umbau der Wirtschaft – man könnte auch sagen: für die Abschaffung des Wirtschaftsstandorts Deutschland –, zum anderen für den Marsch in die...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1189</th>\n",
       "      <td>1188</td>\n",
       "      <td>10</td>\n",
       "      <td>1188_dieselfahrer_fahrverbote_hardwarenachrüstungen_dieselbetrug</td>\n",
       "      <td>[dieselfahrer, fahrverbote, hardwarenachrüstungen, dieselbetrug, hotline, dieselfahrverbote, pflichtnachrüstungen, dieselmodelle, schadstoffverhinderung, großflächigste]</td>\n",
       "      <td>[Herr Präsident! Liebe Kolleginnen und Kollegen! Millionen Dieselfahrer sind verunsichert wegen der Fahrverbote und der immensen Wertverluste. Ein riesiger volkswirtschaftlicher Schaden entsteht. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1190</th>\n",
       "      <td>1189</td>\n",
       "      <td>10</td>\n",
       "      <td>1189_lustiger_hetztiraden_unbeschreiblich_schnaps</td>\n",
       "      <td>[lustiger, hetztiraden, unbeschreiblich, schnaps, furor, losgelassen, bestreite, ablehnende, verantwortungsvolles, kotré]</td>\n",
       "      <td>[Ich wollte heute aber – und ich bleibe meinen Prinzipien treu – Furor und Empörung, Liebe Kolleginnen und Kollegen, immer nur den Schnaps in der Theorie zu versprechen, macht in diesem Land auch ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1191 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Topic   Count  \\\n",
       "0        -1  162398   \n",
       "1         0   27469   \n",
       "2         1   22542   \n",
       "3         2   20201   \n",
       "4         3   18019   \n",
       "...     ...     ...   \n",
       "1186   1185      10   \n",
       "1187   1186      10   \n",
       "1188   1187      10   \n",
       "1189   1188      10   \n",
       "1190   1189      10   \n",
       "\n",
       "                                                                          Name  \\\n",
       "0                                                    -1_herr_antrag_müssen_afd   \n",
       "1                              0_nullkommagarnichts_büttenrede_verwechseln_ach   \n",
       "2                                                                        1____   \n",
       "3                                                                        2____   \n",
       "4                                                                        3____   \n",
       "...                                                                        ...   \n",
       "1186                                1185_auswärtigen_diplomaten_ortsfesten_amt   \n",
       "1187               1186_aushandlungssystem_schimke_tarifverträge_lohnsenkungen   \n",
       "1188  1187_regierungsansprache_wohlstandsindex_hauptgegenstand_einprogrammiert   \n",
       "1189          1188_dieselfahrer_fahrverbote_hardwarenachrüstungen_dieselbetrug   \n",
       "1190                         1189_lustiger_hetztiraden_unbeschreiblich_schnaps   \n",
       "\n",
       "                                                                                                                                                                 Representation  \\\n",
       "0                                                                                                        [herr, antrag, müssen, afd, deutschland, schon, sagen, mal, ja, heute]   \n",
       "1                                                                                                    [nullkommagarnichts, büttenrede, verwechseln, ach, nein, mal, schon, , , ]   \n",
       "2                                                                                                                                                          [, , , , , , , , , ]   \n",
       "3                                                                                                                                                          [, , , , , , , , , ]   \n",
       "4                                                                                                                                                          [, , , , , , , , , ]   \n",
       "...                                                                                                                                                                         ...   \n",
       "1186                                                   [auswärtigen, diplomaten, ortsfesten, amt, spezialisten, amtes, rotation, dienstposten, auslandsbezug, ausländerrechtes]   \n",
       "1187          [aushandlungssystem, schimke, tarifverträge, lohnsenkungen, unternehmenszentralen, auffangböden, bargeldauszahlungen, osten, tarifverträgen, menschenrechtsthema]   \n",
       "1188                  [regierungsansprache, wohlstandsindex, hauptgegenstand, einprogrammiert, premium, dazugeben, einigungsprozesses, ropäische, regionalbanken, partizipiert]   \n",
       "1189  [dieselfahrer, fahrverbote, hardwarenachrüstungen, dieselbetrug, hotline, dieselfahrverbote, pflichtnachrüstungen, dieselmodelle, schadstoffverhinderung, großflächigste]   \n",
       "1190                                                  [lustiger, hetztiraden, unbeschreiblich, schnaps, furor, losgelassen, bestreite, ablehnende, verantwortungsvolles, kotré]   \n",
       "\n",
       "                                                                                                                                                                                          Representative_Docs  \n",
       "0     [– Gut. Jetzt lassen Sie mich doch mal darüber reden., Ich gebe der Opposition recht, die uns vorwirft, dass wir sehr lange gebraucht haben etc. Da haben Sie einen Punkt gemacht. Wir sind in einer...  \n",
       "1                                                                                                                                                                                       [({0}), ({0}), ({0})]  \n",
       "2                                                                                                                                                                                       [({1}), ({1}), ({1})]  \n",
       "3                                                                                                                                                                                       [({2}), ({2}), ({2})]  \n",
       "4                                                                                                                                                                                       [({3}), ({3}), ({3})]  \n",
       "...                                                                                                                                                                                                       ...  \n",
       "1186  [Der diplomatische Dienst ist durch zwei Prinzipien gekennzeichnet: Die Angehörigen des diplomatischen Dienstes müssen zur Rotation bereit sein, und sie verstehen sich als Generalisten. Das heißt ...  \n",
       "1187  [Ein weiterer Punkt ist: Dort, wo wir Regelungen haben – es wurde schon von Frau Schimke genannt, der ich sonst selten zustimme –, beispielsweise bei den Tarifverträgen, bei den Löhnen, haben wir ...  \n",
       "1188  [Die Krise wird benutzt: zum einen, wie erwähnt, für den Umbau der Wirtschaft – man könnte auch sagen: für die Abschaffung des Wirtschaftsstandorts Deutschland –, zum anderen für den Marsch in die...  \n",
       "1189  [Herr Präsident! Liebe Kolleginnen und Kollegen! Millionen Dieselfahrer sind verunsichert wegen der Fahrverbote und der immensen Wertverluste. Ein riesiger volkswirtschaftlicher Schaden entsteht. ...  \n",
       "1190  [Ich wollte heute aber – und ich bleibe meinen Prinzipien treu – Furor und Empörung, Liebe Kolleginnen und Kollegen, immer nur den Schnaps in der Theorie zu versprechen, macht in diesem Land auch ...  \n",
       "\n",
       "[1191 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bertopic import BERTopic\n",
    "\n",
    "umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine')\n",
    "german_stop_words = stopwords.words('german')\n",
    "vectorizer_model = CountVectorizer(stop_words=german_stop_words)\n",
    "\n",
    "topics = np.load(os.path.join(BASE_OUTPUT, \"paraphrase-multilingual-mpnet-base-v2/embeddings_19_topics.npy\"), allow_pickle=True)\n",
    "topic_model = BERTopic.load(os.path.join(BASE_OUTPUT, \"paraphrase-multilingual-mpnet-base-v2/embeddings_19.model\"))\n",
    "# show topics\n",
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d235fe18",
   "metadata": {},
   "source": [
    "## BERTopic Analysis and Comparison to LDA/NMF\n",
    "\n",
    "After applying traditional topic modeling approaches, it can be observed that both models yielded interpretable and semantically meaningful topic representations. The extracted topics typically consisted of clear, thematically coherent keyphrases, and could be readily mapped to real-world political domains or current debates.\n",
    "\n",
    "In contrast, the **BERTopic**-based approach—despite leveraging advanced transformer embeddings—has thus far produced less interpretable results. As seen in the \"get_topic_info()\" output, the automatically assigned topic representations are either dominated by generic, high-frequency tokens (such as \"herr\", \"afd\", \"müssen\", \"antrag\") or occasionally by unrelated or placeholder-like terms (e.g., \"nullkommagarnichts\"). Some topics (e.g., index 1) are even entirely filled with empty strings, suggesting issues either in preprocessing, topic assignment, or model configuration.\n",
    "\n",
    "### **Potential Causes**\n",
    "- **Input granularity:** As we already observed in the first topic modeling notebook, input length varies a lot between speeches. It could eventually help to filter out too short speeches here as well.\n",
    "- **Stopword handling:** Although the CountVectorizer is set up with German stopwords, transformer models themselves still process and embed these words (which are then reflected in topic names and representations).\n",
    "- **Representation calculation:** By not calculating probabilities (to save compute), some topic meta-data (such as topic-word distributions) might be less reliable.\n",
    "- **Embeddings:** The difference between the smaller (\"MiniLM\") and larger (\"mpnet\") embedding models is visible but does not fully solve the interpretability issue, possibly due to above factors.\n",
    "\n",
    "### **Possible solutions**:\n",
    "- **PreProcessing**: Although it was mentioned that this is generally not needed, the result shows us that at least some kind of PreProcessing could be helpful in improving topic's coherence and meaningfullnessanyway.\n",
    "\n",
    "#### Should we compare embeddings-based topic models on data with stopwords removed or lemmatized?\n",
    "As we already preprocessed data for the topic modeling using LDA and NMF, we stored them for later use. Therefore, the question remains if they can help us out here as well.\n",
    "##### 1. **Context Preservation**\n",
    "Transformer models (like BERT or models used in BERTopic) are trained on large corpora with “normal” text, including function words (stopwords) and inflections. They exploit the syntactic and semantic context provided by these words to build their representations.  \n",
    "- **Lemmatization** removes morphological variety, which can lead to loss of subtle meanings or syntactic information that these models leverage.\n",
    "- **Stopword removal** might remove words that, while not individually meaningful, contribute to overall meaning and fluency at the sentence or paragraph level.\n",
    "\n",
    "##### 2. **Intended Use vs. Classic Models**\n",
    "- **Bag-of-words methods (LDA, NMF):** Work best on preprocessed, lemmatized, and stopword-removed texts because they have no understanding of structure or context.\n",
    "- **Embeddings-based models:** Designed to work with raw or lightly cleaned text.\n",
    "\n",
    "##### 3. **Conclusion for the Project**\n",
    "- Re-training the embeddings on the preprocessed data will not lead to a coherent topic modeling, which we are trying to achieve. - Next step will be a combination: A new Embedding model will be trained, again on the paragraphs list (not lemmatized, no stopwords removed). - However, as we previously analyzed 50 as a good minimum length, we will apply that here as well, to filter out speeches which cannot be directly assigned to a topic.\n",
    "- Additionally, CountVectorizer will now use the SpaCy stopwords list, instead of the one provided by NLTK. Because it is bigger and more up-to-date, it might improve the results as well.\n",
    "\n",
    "- We will keep the bigger Embeddings model, as the difference was already visible to a certain point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7055f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip short speeches, less then 50 words\n",
    "\n",
    "def strip_short_speeches(df, min_words=50):\n",
    "\tdf['word_count'] = df['speech_content'].apply(lambda x: len(x.split()))\n",
    "\treturn df[df['word_count'] >= min_words].drop(columns=['word_count'])\n",
    "\n",
    "for term in [19, 20]:\n",
    "    # define paths\n",
    "    input_pickle = os.path.join(BASE_INPUT, f\"speech_content_cleaned_{term}.pkl\")\n",
    "    output_pickle = os.path.join(BASE_OUTPUT, f\"iteration_3/speech_content_stripped_{term}.pkl\")\n",
    "    df = pd.read_pickle(input_pickle)\n",
    "    df = strip_short_speeches(df, min_words=50)\n",
    "    # check first if output directory exists\n",
    "    if not os.path.exists(os.path.dirname(output_pickle)):\n",
    "        os.makedirs(os.path.dirname(output_pickle))\n",
    "    df.to_pickle(output_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33bd7c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings already exist at ../dataProcessedStage/topicModeling/bertopic/iteration_3/embeddings_19.npy, skipping training.\n",
      "Training BERTopic model...\n",
      "len(paragraphs): 459009\n",
      "embeddings.shape: (459009, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-21 18:12:37,598 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-06-21 18:31:46,193 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-06-21 18:31:46,230 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2025-06-21 18:32:25,864 - BERTopic - Cluster - Completed ✓\n",
      "2025-06-21 18:32:25,942 - BERTopic - Representation - Fine-tuning topics using representation models.\n"
     ]
    },
    {
     "ename": "InvalidParameterError",
     "evalue": "The 'stop_words' parameter of CountVectorizer must be a str among {'english'}, an instance of 'list' or None. Got {'ag', 'davor', 'ihre', 'rechter', 'erster', 'teil', 'na', 'etwas', 'jedem', 'daneben', 'eben', 'seinem', 'geht', 'vierte', 'danach', 'dass', 'irgend', 'sind', 'immer', 'dessen', 'dort', 'hätte', 'recht', 'tagen', 'gewesen', 'muss', 'erste', 'grosses', 'wart', 'derselben', 'vielen', 'wann', 'gemocht', 'zum', 'zugleich', 'besonders', 'um', 'je', 'kleinen', 'demgemäss', 'jenes', 'manches', 'acht', 'sechsten', 'viel', 'demgemäß', 'wo', 'ganzen', 'nachdem', 'eine', 'dieser', 'neunten', 'derjenigen', 'alle', 'welchem', 'könnt', 'hoch', 'wollten', 'wenigstens', 'zwei', 'sechster', 'ersten', 'rechte', 'auf', 'denselben', 'ihm', 'großer', 'drittes', 'einmal', 'bereits', 'sich', 'eigene', 'welcher', 'seit', 'manchem', 'diejenige', 'daran', 'los', 'vierten', 'wenn', 'gehabt', 'siebenter', 'nicht', 'alles', 'ende', 'selbst', 'uhr', 'übrigens', 'wollt', 'dritter', 'auch', 'her', 'natürlich', 'durften', 'hatten', 'daher', 'tat', 'ob', 'nur', 'so', 'manchen', 'vier', 'jahr', 'früher', 'man', 'eigener', 'deshalb', 'sie', 'geschweige', 'außer', 'dritte', 'siebenten', 'oben', 'ehrlich', 'machte', 'viele', 'meine', 'weniger', 'kannst', 'würde', 'ganzes', 'weil', 'dein', 'damit', 'von', 'das', 'seine', 'gute', 'als', 'durfte', 'durchaus', 'ihrem', 'bei', 'kleines', 'sieben', 'siebter', 'dieses', 'macht', 'mich', 'dafür', 'derselbe', 'einige', 'vergangene', 'gekannt', 'hast', 'willst', 'zehnten', 'deswegen', 'jenen', 'war', 'unserer', 'an', 'grossen', 'solcher', 'sollen', 'demzufolge', 'deren', 'wollen', 'oft', 'dagegen', 'dahinter', 'später', 'dementsprechend', 'allein', 'sein', 'über', 'darüber', 'gegen', 'ohne', 'a', 'wohl', 'daselbst', 'fünfter', 'drei', 'diesem', 'solang', 'gemacht', 'dasein', 'rechtes', 'wieder', 'beiden', 'weiteres', 'kommt', 'erstes', 'gutes', 'aber', 'große', 'achtes', 'sonst', 'ach', 'bald', 'niemandem', 'offen', 'daß', 'seiner', 'sollte', 'geworden', 'besser', 'werdet', 'dazwischen', 'anderem', 'morgen', 'bis', 'mancher', 'möchte', 'ab', 'seien', 'darauf', 'rechten', 'anderen', 'dahin', 'allgemeinen', 'weiter', 'achte', 'zeit', 'also', 'zur', 'weitere', 'kommen', 'dich', 'solchem', 'mehr', 'gibt', 'kann', 'leider', 'hätten', 'dank', 'weit', 'neunte', 'würden', 'jemanden', 'grosser', 'magst', 'mag', 'leicht', 'lang', 'siebentes', 'solchen', 'dermaßen', 'gegenüber', 'solche', 'deine', 'wie', 'neben', 'uns', 'wen', 'warum', 'gleich', 'neue', 'groß', 'müssen', 'demgegenüber', 'sechste', 'kein', 'großen', 'du', 'jetzt', 'damals', 'siebtes', 'zwar', 'soll', 'vom', 'bisher', 'dieselben', 'wer', 'im', 'siebte', 'hin', 'musst', 'davon', 'da', 'einen', 'grosse', 'ganze', 'andern', 'viertes', 'jedermanns', 'sagte', 'zuerst', 'demselben', 'kurz', 'kaum', 'einiges', 'kleiner', 'richtig', 'jemandem', 'währenddem', 'vierter', 'neun', 'ihr', 'eigenes', 'ich', 'diejenigen', 'darum', 'deiner', 'fünf', 'mögen', 'mittel', 'infolgedessen', 'keinem', 'denen', 'jemand', 'nichts', 'sah', 'zwischen', 'gemusst', 'ist', 'haben', 'seitdem', 'dermassen', 'vergangenen', 'jedoch', 'was', 'ihrer', 'wirst', 'seid', 'noch', 'gehen', 'während', 'welche', 'wird', 'sagt', 'bekannt', 'jenem', 'darfst', 'neuen', 'besten', 'allen', 'ein', 'einer', 'diese', 'nun', 'er', 'will', 'keinen', 'einem', 'allerdings', 'jedermann', 'elf', 'mir', 'ganz', 'heisst', 'zunächst', 'seinen', 'für', 'mussten', 'entweder', 'keiner', 'beim', 'konnten', 'niemanden', 'heißt', 'zusammen', 'gab', 'ihres', 'zweiten', 'wäre', 'wurde', 'rund', 'sechs', 'mochten', 'sechstes', 'dadurch', 'jede', 'fünften', 'ausserdem', 'genug', 'fünfte', 'nach', 'guter', 'lange', 'statt', 'währenddessen', 'gerade', 'vielleicht', 'satt', 'dasselbe', 'solches', 'sehr', 'siebten', 'mochte', 'schlecht', 'jeder', 'diesen', 'ihren', 'ebenso', 'durch', 'überhaupt', 'eigenen', 'weniges', 'darf', 'nahm', 'endlich', 'dabei', 'niemand', 'einmaleins', 'muß', 'dies', 'wem', 'dürfen', 'neunter', 'drin', 'mögt', 'gekonnt', 'deinem', 'wollte', 'heute', 'wenig', 'ja', 'und', 'aus', 'kam', 'außerdem', 'etwa', 'wirklich', 'ganzer', 'gedurft', 'jene', 'schon', 'aller', 'trotzdem', 'jahre', 'seines', 'weiteren', 'müsst', 'zehn', 'sei', 'ging', 'dann', 'mit', 'es', 'jener', 'werden', 'zehnte', 'gesagt', 'dieselbe', 'werde', 'habt', 'fünftes', 'können', 'zweite', 'sondern', 'achten', 'die', 'siebente', 'am', 'worden', 'sowie', 'einander', 'zehnter', 'zu', 'doch', 'á', 'daraus', 'sollten', 'beispiel', 'zehntes', 'waren', 'darin', 'bist', 'zwanzig', 'derjenige', 'dritten', 'gross', 'manche', 'gewollt', 'eigen', 'einigen', 'den', 'denn', 'tun', 'darunter', 'wir', 'hier', 'zweiter', 'hinter', 'einiger', 'hatte', 'meinen', 'en', 'mein', 'indem', 'tel', 'allem', 'ihn', 'vor', 'ins', 'lieber', 'dir', 'wahr', 'euch', 'welches', 'meines', 'jeden', 'unter', 'anders', 'tage', 'desselben', 'in', 'oder', 'konnte', 'meiner', 'nein', 'keine', 'der', 'musste', 'nie', 'möglich', 'jahren', 'welchen', 'vielem', 'großes', 'gut', 'neuntes', 'tag', 'bin', 'machen', 'beide', 'des', 'andere', 'dazu', 'könnte', 'eines', 'hat', 'kleine', 'erst', 'dem', 'ausser', 'achter', 'habe', 'unsere', 'gar', 'ihnen', 'meinem', 'zurück', 'dürft', 'gern', 'unser', 'zweites', 'wurden', 'wessen', 'wenige', 'wegen'} instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInvalidParameterError\u001b[39m                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m input_pickle = os.path.join(BASE_OUTPUT, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33miteration_3/speech_content_stripped_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mterm\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.pkl\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m output_npy = os.path.join(BASE_OUTPUT, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33miteration_3/embeddings_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mterm\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.npy\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[43mtrain_bertopic\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_pickle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_npy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparaphrase-multilingual-mpnet-base-v2\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvectorizer_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspacy_vectorizer_model\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 47\u001b[39m, in \u001b[36mtrain_bertopic\u001b[39m\u001b[34m(input_pickle, output_npy, model, vectorizer_model)\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mlen(paragraphs): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(paragraphs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     46\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33membeddings.shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00membeddings.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m topics = \u001b[43mtopic_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparagraphs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# save the model\u001b[39;00m\n\u001b[32m     49\u001b[39m topic_model.save(output_npy.replace(\u001b[33m'\u001b[39m\u001b[33m.npy\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m.model\u001b[39m\u001b[33m'\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.9/envs/nlp/lib/python3.12/site-packages/bertopic/_bertopic.py:515\u001b[39m, in \u001b[36mBERTopic.fit_transform\u001b[39m\u001b[34m(self, documents, embeddings, images, y)\u001b[39m\n\u001b[32m    511\u001b[39m     \u001b[38;5;28mself\u001b[39m._save_representative_docs(custom_documents)\n\u001b[32m    513\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    514\u001b[39m     \u001b[38;5;66;03m# Extract topics by calculating c-TF-IDF, reduce topics if needed, and get representations.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m515\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_extract_topics\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    516\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfine_tune_representation\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnr_topics\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    518\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.nr_topics:\n\u001b[32m    519\u001b[39m         documents = \u001b[38;5;28mself\u001b[39m._reduce_topics(documents)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.9/envs/nlp/lib/python3.12/site-packages/bertopic/_bertopic.py:4030\u001b[39m, in \u001b[36mBERTopic._extract_topics\u001b[39m\u001b[34m(self, documents, embeddings, mappings, verbose, fine_tune_representation)\u001b[39m\n\u001b[32m   4027\u001b[39m     logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRepresentation - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maction\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m topics using \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   4029\u001b[39m documents_per_topic = documents.groupby([\u001b[33m\"\u001b[39m\u001b[33mTopic\u001b[39m\u001b[33m\"\u001b[39m], as_index=\u001b[38;5;28;01mFalse\u001b[39;00m).agg({\u001b[33m\"\u001b[39m\u001b[33mDocument\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m.join})\n\u001b[32m-> \u001b[39m\u001b[32m4030\u001b[39m \u001b[38;5;28mself\u001b[39m.c_tf_idf_, words = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_c_tf_idf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments_per_topic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4031\u001b[39m \u001b[38;5;28mself\u001b[39m.topic_representations_ = \u001b[38;5;28mself\u001b[39m._extract_words_per_topic(\n\u001b[32m   4032\u001b[39m     words,\n\u001b[32m   4033\u001b[39m     documents,\n\u001b[32m   4034\u001b[39m     fine_tune_representation=fine_tune_representation,\n\u001b[32m   4035\u001b[39m     calculate_aspects=fine_tune_representation,\n\u001b[32m   4036\u001b[39m )\n\u001b[32m   4037\u001b[39m \u001b[38;5;28mself\u001b[39m._create_topic_vectors(documents=documents, embeddings=embeddings, mappings=mappings)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.9/envs/nlp/lib/python3.12/site-packages/bertopic/_bertopic.py:4247\u001b[39m, in \u001b[36mBERTopic._c_tf_idf\u001b[39m\u001b[34m(self, documents_per_topic, fit, partial_fit)\u001b[39m\n\u001b[32m   4245\u001b[39m     X = \u001b[38;5;28mself\u001b[39m.vectorizer_model.partial_fit(documents).update_bow(documents)\n\u001b[32m   4246\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m fit:\n\u001b[32m-> \u001b[39m\u001b[32m4247\u001b[39m     X = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvectorizer_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4248\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   4249\u001b[39m     X = \u001b[38;5;28mself\u001b[39m.vectorizer_model.transform(documents)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.9/envs/nlp/lib/python3.12/site-packages/sklearn/base.py:1382\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1377\u001b[39m partial_fit_and_fitted = (\n\u001b[32m   1378\u001b[39m     fit_method.\u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33mpartial_fit\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m _is_fitted(estimator)\n\u001b[32m   1379\u001b[39m )\n\u001b[32m   1381\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m global_skip_validation \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m partial_fit_and_fitted:\n\u001b[32m-> \u001b[39m\u001b[32m1382\u001b[39m     \u001b[43mestimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_validate_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m   1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, *args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.9/envs/nlp/lib/python3.12/site-packages/sklearn/base.py:436\u001b[39m, in \u001b[36mBaseEstimator._validate_params\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    428\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_validate_params\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    429\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Validate types and values of constructor parameters\u001b[39;00m\n\u001b[32m    430\u001b[39m \n\u001b[32m    431\u001b[39m \u001b[33;03m    The expected type and values must be defined in the `_parameter_constraints`\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    434\u001b[39m \u001b[33;03m    accepted constraints.\u001b[39;00m\n\u001b[32m    435\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m436\u001b[39m     \u001b[43mvalidate_parameter_constraints\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    437\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_parameter_constraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    438\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcaller_name\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__class__\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    440\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.9/envs/nlp/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:98\u001b[39m, in \u001b[36mvalidate_parameter_constraints\u001b[39m\u001b[34m(parameter_constraints, params, caller_name)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     93\u001b[39m     constraints_str = (\n\u001b[32m     94\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join([\u001b[38;5;28mstr\u001b[39m(c)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mconstraints[:-\u001b[32m1\u001b[39m]])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m or\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     95\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints[-\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     96\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m InvalidParameterError(\n\u001b[32m     99\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m parameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcaller_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_val\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    101\u001b[39m )\n",
      "\u001b[31mInvalidParameterError\u001b[39m: The 'stop_words' parameter of CountVectorizer must be a str among {'english'}, an instance of 'list' or None. Got {'ag', 'davor', 'ihre', 'rechter', 'erster', 'teil', 'na', 'etwas', 'jedem', 'daneben', 'eben', 'seinem', 'geht', 'vierte', 'danach', 'dass', 'irgend', 'sind', 'immer', 'dessen', 'dort', 'hätte', 'recht', 'tagen', 'gewesen', 'muss', 'erste', 'grosses', 'wart', 'derselben', 'vielen', 'wann', 'gemocht', 'zum', 'zugleich', 'besonders', 'um', 'je', 'kleinen', 'demgemäss', 'jenes', 'manches', 'acht', 'sechsten', 'viel', 'demgemäß', 'wo', 'ganzen', 'nachdem', 'eine', 'dieser', 'neunten', 'derjenigen', 'alle', 'welchem', 'könnt', 'hoch', 'wollten', 'wenigstens', 'zwei', 'sechster', 'ersten', 'rechte', 'auf', 'denselben', 'ihm', 'großer', 'drittes', 'einmal', 'bereits', 'sich', 'eigene', 'welcher', 'seit', 'manchem', 'diejenige', 'daran', 'los', 'vierten', 'wenn', 'gehabt', 'siebenter', 'nicht', 'alles', 'ende', 'selbst', 'uhr', 'übrigens', 'wollt', 'dritter', 'auch', 'her', 'natürlich', 'durften', 'hatten', 'daher', 'tat', 'ob', 'nur', 'so', 'manchen', 'vier', 'jahr', 'früher', 'man', 'eigener', 'deshalb', 'sie', 'geschweige', 'außer', 'dritte', 'siebenten', 'oben', 'ehrlich', 'machte', 'viele', 'meine', 'weniger', 'kannst', 'würde', 'ganzes', 'weil', 'dein', 'damit', 'von', 'das', 'seine', 'gute', 'als', 'durfte', 'durchaus', 'ihrem', 'bei', 'kleines', 'sieben', 'siebter', 'dieses', 'macht', 'mich', 'dafür', 'derselbe', 'einige', 'vergangene', 'gekannt', 'hast', 'willst', 'zehnten', 'deswegen', 'jenen', 'war', 'unserer', 'an', 'grossen', 'solcher', 'sollen', 'demzufolge', 'deren', 'wollen', 'oft', 'dagegen', 'dahinter', 'später', 'dementsprechend', 'allein', 'sein', 'über', 'darüber', 'gegen', 'ohne', 'a', 'wohl', 'daselbst', 'fünfter', 'drei', 'diesem', 'solang', 'gemacht', 'dasein', 'rechtes', 'wieder', 'beiden', 'weiteres', 'kommt', 'erstes', 'gutes', 'aber', 'große', 'achtes', 'sonst', 'ach', 'bald', 'niemandem', 'offen', 'daß', 'seiner', 'sollte', 'geworden', 'besser', 'werdet', 'dazwischen', 'anderem', 'morgen', 'bis', 'mancher', 'möchte', 'ab', 'seien', 'darauf', 'rechten', 'anderen', 'dahin', 'allgemeinen', 'weiter', 'achte', 'zeit', 'also', 'zur', 'weitere', 'kommen', 'dich', 'solchem', 'mehr', 'gibt', 'kann', 'leider', 'hätten', 'dank', 'weit', 'neunte', 'würden', 'jemanden', 'grosser', 'magst', 'mag', 'leicht', 'lang', 'siebentes', 'solchen', 'dermaßen', 'gegenüber', 'solche', 'deine', 'wie', 'neben', 'uns', 'wen', 'warum', 'gleich', 'neue', 'groß', 'müssen', 'demgegenüber', 'sechste', 'kein', 'großen', 'du', 'jetzt', 'damals', 'siebtes', 'zwar', 'soll', 'vom', 'bisher', 'dieselben', 'wer', 'im', 'siebte', 'hin', 'musst', 'davon', 'da', 'einen', 'grosse', 'ganze', 'andern', 'viertes', 'jedermanns', 'sagte', 'zuerst', 'demselben', 'kurz', 'kaum', 'einiges', 'kleiner', 'richtig', 'jemandem', 'währenddem', 'vierter', 'neun', 'ihr', 'eigenes', 'ich', 'diejenigen', 'darum', 'deiner', 'fünf', 'mögen', 'mittel', 'infolgedessen', 'keinem', 'denen', 'jemand', 'nichts', 'sah', 'zwischen', 'gemusst', 'ist', 'haben', 'seitdem', 'dermassen', 'vergangenen', 'jedoch', 'was', 'ihrer', 'wirst', 'seid', 'noch', 'gehen', 'während', 'welche', 'wird', 'sagt', 'bekannt', 'jenem', 'darfst', 'neuen', 'besten', 'allen', 'ein', 'einer', 'diese', 'nun', 'er', 'will', 'keinen', 'einem', 'allerdings', 'jedermann', 'elf', 'mir', 'ganz', 'heisst', 'zunächst', 'seinen', 'für', 'mussten', 'entweder', 'keiner', 'beim', 'konnten', 'niemanden', 'heißt', 'zusammen', 'gab', 'ihres', 'zweiten', 'wäre', 'wurde', 'rund', 'sechs', 'mochten', 'sechstes', 'dadurch', 'jede', 'fünften', 'ausserdem', 'genug', 'fünfte', 'nach', 'guter', 'lange', 'statt', 'währenddessen', 'gerade', 'vielleicht', 'satt', 'dasselbe', 'solches', 'sehr', 'siebten', 'mochte', 'schlecht', 'jeder', 'diesen', 'ihren', 'ebenso', 'durch', 'überhaupt', 'eigenen', 'weniges', 'darf', 'nahm', 'endlich', 'dabei', 'niemand', 'einmaleins', 'muß', 'dies', 'wem', 'dürfen', 'neunter', 'drin', 'mögt', 'gekonnt', 'deinem', 'wollte', 'heute', 'wenig', 'ja', 'und', 'aus', 'kam', 'außerdem', 'etwa', 'wirklich', 'ganzer', 'gedurft', 'jene', 'schon', 'aller', 'trotzdem', 'jahre', 'seines', 'weiteren', 'müsst', 'zehn', 'sei', 'ging', 'dann', 'mit', 'es', 'jener', 'werden', 'zehnte', 'gesagt', 'dieselbe', 'werde', 'habt', 'fünftes', 'können', 'zweite', 'sondern', 'achten', 'die', 'siebente', 'am', 'worden', 'sowie', 'einander', 'zehnter', 'zu', 'doch', 'á', 'daraus', 'sollten', 'beispiel', 'zehntes', 'waren', 'darin', 'bist', 'zwanzig', 'derjenige', 'dritten', 'gross', 'manche', 'gewollt', 'eigen', 'einigen', 'den', 'denn', 'tun', 'darunter', 'wir', 'hier', 'zweiter', 'hinter', 'einiger', 'hatte', 'meinen', 'en', 'mein', 'indem', 'tel', 'allem', 'ihn', 'vor', 'ins', 'lieber', 'dir', 'wahr', 'euch', 'welches', 'meines', 'jeden', 'unter', 'anders', 'tage', 'desselben', 'in', 'oder', 'konnte', 'meiner', 'nein', 'keine', 'der', 'musste', 'nie', 'möglich', 'jahren', 'welchen', 'vielem', 'großes', 'gut', 'neuntes', 'tag', 'bin', 'machen', 'beide', 'des', 'andere', 'dazu', 'könnte', 'eines', 'hat', 'kleine', 'erst', 'dem', 'ausser', 'achter', 'habe', 'unsere', 'gar', 'ihnen', 'meinem', 'zurück', 'dürft', 'gern', 'unser', 'zweites', 'wurden', 'wessen', 'wenige', 'wegen'} instead."
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "spacy_instance = spacy.load(\"de_core_news_sm\")\n",
    "german_stop_words = spacy_instance.Defaults.stop_words\n",
    "spacy_vectorizer_model = CountVectorizer(stop_words=german_stop_words)\n",
    "\n",
    "for term in [19, 20]:\n",
    "    # define paths\n",
    "    input_pickle = os.path.join(BASE_OUTPUT, f\"iteration_3/speech_content_stripped_{term}.pkl\")\n",
    "    output_npy = os.path.join(BASE_OUTPUT, f\"iteration_3/embeddings_{term}.npy\")\n",
    "    train_bertopic(input_pickle, output_npy, model=\"paraphrase-multilingual-mpnet-base-v2\", vectorizer_model=spacy_vectorizer_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f2948e",
   "metadata": {},
   "source": [
    "## Error\n",
    "Different from NLTK, spacy gives the stopwords as a set, not a list. We have to convert it therefore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "68f83140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings already exist at ../dataProcessedStage/topicModeling/bertopic/iteration_3/embeddings_19.npy, skipping training.\n",
      "BERTopic model already exists at ../dataProcessedStage/topicModeling/bertopic/iteration_3/embeddings_19.model, skipping training.\n",
      "Embeddings already exist at ../dataProcessedStage/topicModeling/bertopic/iteration_3/embeddings_20.npy, skipping training.\n",
      "BERTopic model already exists at ../dataProcessedStage/topicModeling/bertopic/iteration_3/embeddings_20.model, skipping training.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "spacy_instance = spacy.load(\"de_core_news_sm\")\n",
    "german_stop_words = list(spacy_instance.Defaults.stop_words)\n",
    "spacy_vectorizer_model = CountVectorizer(stop_words=german_stop_words)\n",
    "\n",
    "for term in [19, 20]:\n",
    "    # define paths\n",
    "    input_pickle = os.path.join(BASE_OUTPUT, f\"iteration_3/speech_content_stripped_{term}.pkl\")\n",
    "    output_npy = os.path.join(BASE_OUTPUT, f\"iteration_3/embeddings_{term}.npy\")\n",
    "    train_bertopic(input_pickle, output_npy, model=\"paraphrase-multilingual-mpnet-base-v2\", vectorizer_model=spacy_vectorizer_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "35828d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topic   Count                                     Name  \\\n",
      "0     -1  167329           -1_herr_afd_deutschland_antrag   \n",
      "1      0   24676            0_büttenrede_verwechseln_mal_   \n",
      "2      1   22262                                    1____   \n",
      "3      2   20179                                    2____   \n",
      "4      3   18017                        3_aufhören_ziel__   \n",
      "5      4   15750                                    4____   \n",
      "6      5   13440                   5_oh_nennt_arzt_punkte   \n",
      "7      6   11139     6_basispunkte_kumuliert_zuruf_gründe   \n",
      "8      7    9058  7_thebaner_kundigen_biblische_testament   \n",
      "9      8    7273                                  8_uh___   \n",
      "\n",
      "                                      Representation  \\\n",
      "0  [herr, afd, deutschland, antrag, sagen, mal, k...   \n",
      "1       [büttenrede, verwechseln, mal, , , , , , , ]   \n",
      "2                               [, , , , , , , , , ]   \n",
      "3                               [, , , , , , , , , ]   \n",
      "4                   [aufhören, ziel, , , , , , , , ]   \n",
      "5                               [, , , , , , , , , ]   \n",
      "6              [oh, nennt, arzt, punkte, , , , , , ]   \n",
      "7  [basispunkte, kumuliert, zuruf, gründe, nenne,...   \n",
      "8  [thebaner, kundigen, biblische, testament, hin...   \n",
      "9                             [uh, , , , , , , , , ]   \n",
      "\n",
      "                                 Representative_Docs  \n",
      "0  [Das reicht nicht aus. Nach wie vor gibt es er...  \n",
      "1  [({0}), ({0}), ({0}): Das kann man schon mal v...  \n",
      "2                              [({1}), ({1}), ({1})]  \n",
      "3                              [({2}), ({2}), ({2})]  \n",
      "4                              [({3}), ({3}), ({3})]  \n",
      "5                              [({4}), ({4}), ({4})]  \n",
      "6                              [({5}), ({5}), ({5})]  \n",
      "7                              [({6}), ({6}), ({6})]  \n",
      "8                              [({7}), ({7}), ({7})]  \n",
      "9                              [({8}), ({8}), ({8})]  \n"
     ]
    }
   ],
   "source": [
    "from bertopic import BERTopic\n",
    "\n",
    "umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine')\n",
    "german_stop_words = stopwords.words('german')\n",
    "vectorizer_model = CountVectorizer(stop_words=german_stop_words)\n",
    "\n",
    "topics = np.load(os.path.join(BASE_OUTPUT, \"iteration_3/embeddings_19_topics.npy\"), allow_pickle=True)\n",
    "topic_model = BERTopic.load(os.path.join(BASE_OUTPUT, \"iteration_3/embeddings_19.model\"))\n",
    "# show topics\n",
    "topic_info = topic_model.get_topic_info()\n",
    "print(topic_info.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b309c62f",
   "metadata": {},
   "source": [
    "### Analysis of Empty or Low-Content Topics in BERTopic\n",
    "\n",
    "Several topics with either blank or underscored labels and no significant representative words can be observed here. This phenomenon typically appears in the `topic_info` table, where certain topics are denoted by names such as `2____` and a largely empty word list under `Representation`.\n",
    "\n",
    "This issue arises when the cluster of documents assigned to a topic lacks cohesive, meaningful vocabulary after stopword removal and other preprocessing. In the specific context of parliamentary records, it is mainly attributable to recurring formal phrases, greetings, speaker tags, or ultra-short utterances (such as approvals or heckles). When such segments are grouped into a topic, they fail to yield distinguishing keywords for the vectorizer, resulting in apparently \"empty\" or non-interpretable topics.\n",
    "\n",
    "From a methodological perspective, these empty topics are important diagnostics:\n",
    "\n",
    "- **They indicate the presence of noise or non-informative text that remains after initial preprocessing.**\n",
    "- **They highlight limitations of the model and vectorization settings—for instance, overly aggressive min_df, max_features filters, or insufficient cleansing of domain-specific boilerplate.**\n",
    "\n",
    "**Actions to take next:**\n",
    "\n",
    "- We will use the same embeddings model.\n",
    "- However, now we define a fixed number of topics - we use 15 in this case, as it was already considered as a good number in the previous notebook.\n",
    "- We use BERTopic's c-tf-idf model to reduce more stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9db962ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bertopic(input_pickle, output_npy, embeddings_model, vectorizer_model,\n",
    "                   min_topic_size=50, nr_topics=None, calculate_probabilities=False, umap_model=None, ctfidf_model=None):\n",
    "    df = pd.read_pickle(input_pickle)\n",
    "    paragraphs = [\n",
    "        paragraph.strip()\n",
    "        for paragraphs in df['speech_content'].str.split('\\n')\n",
    "        for paragraph in paragraphs\n",
    "        if paragraph.strip()\n",
    "    ]\n",
    "    # Embeddings\n",
    "    if os.path.exists(output_npy):\n",
    "        embeddings = np.load(output_npy)\n",
    "        print(f\"Embeddings already exist at {output_npy}, skipping training.\")\n",
    "    else:\n",
    "        print(f\"Training embeddings for {len(paragraphs)} paragraphs...\")\n",
    "        embeddings = train_embeddings(output_npy, paragraphs, embeddings_model)\n",
    "\n",
    "    # BERTopic Model Check\n",
    "    if os.path.exists(output_npy.replace('.npy', '.model')):\n",
    "        print(f\"BERTopic model already exists at {output_npy.replace('.npy', '.model')}, skipping training.\")\n",
    "        return\n",
    "\n",
    "    if not os.path.exists(os.path.dirname(output_npy)):\n",
    "        os.makedirs(os.path.dirname(output_npy))\n",
    "        \n",
    "    # UMAP Model Fallback/Default\n",
    "    if umap_model is None:\n",
    "        from umap import UMAP\n",
    "        umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine')\n",
    "\n",
    "    print(f\"Training BERTopic model (min_topic_size={min_topic_size}, nr_topics={nr_topics})...\")\n",
    "    \n",
    "    topic_model = BERTopic(\n",
    "        vectorizer_model=vectorizer_model,\n",
    "        language=\"german\",\n",
    "        verbose=True,\n",
    "        calculate_probabilities=calculate_probabilities,\n",
    "        umap_model=umap_model,\n",
    "        min_topic_size=min_topic_size,\n",
    "        nr_topics=nr_topics,\n",
    "        ctfidf_model=ctfidf_model\n",
    "                )\n",
    "    print(f\"len(paragraphs): {len(paragraphs)}\")\n",
    "    print(f\"embeddings.shape: {embeddings.shape}\")\n",
    "    topics = topic_model.fit_transform(paragraphs, embeddings=embeddings)\n",
    "    topic_model.save(output_npy.replace('.npy', '.model'))\n",
    "    np.save(output_npy.replace('.npy', '_topics.npy'), topics)\n",
    "    print(f\"BERTopic model trained and saved to {output_npy.replace('.npy', '.model')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4cd2ef0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-22 22:40:30,515 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings already exist at bertopic/iteration_3/embeddings_19.npy, skipping training.\n",
      "Training BERTopic model (min_topic_size=50, nr_topics=15)...\n",
      "len(paragraphs): 459009\n",
      "embeddings.shape: (459009, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-22 23:07:01,316 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-06-22 23:07:01,337 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-06-22 23:07:46,674 - BERTopic - Cluster - Completed ✓\n",
      "2025-06-22 23:07:46,676 - BERTopic - Representation - Extracting topics using c-TF-IDF for topic reduction.\n",
      "2025-06-22 23:07:53,223 - BERTopic - Representation - Completed ✓\n",
      "2025-06-22 23:07:53,242 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2025-06-22 23:07:53,433 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-06-22 23:07:58,428 - BERTopic - Representation - Completed ✓\n",
      "2025-06-22 23:07:58,452 - BERTopic - Topic reduction - Reduced number of topics from 296 to 15\n",
      "2025-06-22 23:07:58,613 - BERTopic - WARNING: When you use `pickle` to save/load a BERTopic model,please make sure that the environments in which you saveand load the model are **exactly** the same. The version of BERTopic,its dependencies, and python need to remain the same.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTopic model trained and saved to bertopic/iteration_3/embeddings_19.model\n"
     ]
    }
   ],
   "source": [
    "spacy_vectorizer_model = CountVectorizer(stop_words=german_stop_words,\n",
    "                                         min_df=5,\n",
    "                                                                                 max_df=0.99)\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\n",
    "\n",
    "for term in [19]:\n",
    "    # define paths\n",
    "    input_pickle = os.path.join(BASE_OUTPUT, f\"iteration_3/speech_content_stripped_{term}.pkl\")\n",
    "    output_npy = os.path.join(BASE_OUTPUT, f\"iteration_3/embeddings_{term}.npy\")\n",
    "    train_bertopic(input_pickle, output_npy, embeddings_model=\"paraphrase-multilingual-mpnet-base-v2\", vectorizer_model=spacy_vectorizer_model, nr_topics=15, ctfidf_model=ctfidf_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd7b9465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topic   Count                                   Name  \\\n",
      "0     -1  141259          -1_herr_müssen_deutschland_ja   \n",
      "1      0  169781                          0_10_11_12_13   \n",
      "2      1   53074  1_soldaten_bundeswehr_europa_russland   \n",
      "3      2   52801             2_euro_mehr_müssen_prozent   \n",
      "4      3   14008         3_dank_vielen_herzlichen_danke   \n",
      "5      4   12793      4_pflege_patienten_pandemie_virus   \n",
      "6      5    6736        5_frauen_familien_kinder_eltern   \n",
      "7      6    3589     6_präsident_präsidentin_herr_liebe   \n",
      "8      7    2562        7_sea_guardian_tourismus_kultur   \n",
      "9      8    1670            8_lehnen_ablehnen_ab_antrag   \n",
      "\n",
      "                                      Representation  \\\n",
      "0  [herr, müssen, deutschland, ja, mehr, kollegen...   \n",
      "1           [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]   \n",
      "2  [soldaten, bundeswehr, europa, russland, deuts...   \n",
      "3  [euro, mehr, müssen, prozent, bildung, brauche...   \n",
      "4  [dank, vielen, herzlichen, danke, aufmerksamke...   \n",
      "5  [pflege, patienten, pandemie, virus, menschen,...   \n",
      "6  [frauen, familien, kinder, eltern, kind, kinde...   \n",
      "7  [präsident, präsidentin, herr, liebe, kollegen...   \n",
      "8  [sea, guardian, tourismus, kultur, mittelmeer,...   \n",
      "9  [lehnen, ablehnen, ab, antrag, nein, anträge, ...   \n",
      "\n",
      "                                 Representative_Docs  \n",
      "0  [Ja, wir werden ein Konjunkturprogramm brauche...  \n",
      "1                           [({10}), ({10}), ({10})]  \n",
      "2  [Ich will deshalb viel mehr über zukunftsgewan...  \n",
      "3  [Sehr geehrter Herr Präsident! Sehr geehrte Ko...  \n",
      "4  [Vielen herzlichen Dank., Vielen herzlichen Da...  \n",
      "5  [Wir haben ja in der Debatte auch immer die Kr...  \n",
      "6  [Der zweite Punkt sind natürlich Teilzeitarbei...  \n",
      "7  [Danke, Herr Präsident. – Meine sehr verehrten...  \n",
      "8  [Sehr geehrte Damen und Herren! Sehr geehrter ...  \n",
      "9  [Deswegen lehnen wir Ihren Antrag ab., Deswege...  \n"
     ]
    }
   ],
   "source": [
    "from bertopic import BERTopic\n",
    "\n",
    "umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine')\n",
    "german_stop_words = stopwords.words('german')\n",
    "vectorizer_model = CountVectorizer(stop_words=german_stop_words)\n",
    "\n",
    "topics = np.load(os.path.join(BASE_OUTPUT, \"iteration_3/embeddings_19_topics.npy\"), allow_pickle=True)\n",
    "topic_model = BERTopic.load(os.path.join(BASE_OUTPUT, \"iteration_3/embeddings_19.model\"))\n",
    "# show topics\n",
    "topic_info = topic_model.get_topic_info()\n",
    "print(topic_info.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ce1326",
   "metadata": {},
   "source": [
    "## Summary and evaluation\n",
    "As can be seen in all of our iterations, we got closer to coherent, meaningful topics. However, we still have a lot of noise, and to a certain points stopwords, in the topics.\n",
    "\n",
    "Other than expected, we **had to preprocess** the data, although we didn't need lemmatization or punctuation removal.\n",
    "\n",
    "It is clearly visible how training an embeddings model makes a difference here, however in all iterations, a lot of time and computation power was used.\n",
    "\n",
    "In the end, we saw that both approaches lead to a topic overview."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
